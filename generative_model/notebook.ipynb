{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini projekt - modele generatywne\n",
    "Due May 13, 2024 4:00 PM\n",
    "\n",
    "Tym razem zadanie będzie polegało na stworzeniu modelu generatywnego który generował będzie nowe obrazki przedstawiające znaki drogowe. Do wyboru mają Państwo dowolny model generatywny (VAE, GAN, GLOW, VAEGAN, czy modele dyfuzyjne. W przypadku tych ostatnich polecam zapoznanie się z takim notebookiem: https://huggingface.co/learn/diffusion-course/en/unit1/3, jeżeli będą Państwo chętni to możemy wspólnie przejść przez niego w ramach laborek konsultacyjnych.\n",
    "\n",
    "Zbiór danych udostępniłem Państwu przez onedrive (trafic_32.zip) i ma taką samą strukturę jak poprzednio (zgodną z domyślnymi ustawieniami ImageFolderu). Znaki podzielone są na klasy, które jak najbardziej mogą Państwo wykorzystywać do generowania próbek. Tym razem zamiast predykcji proszę o zwrócenie mi kodu z implementacją eksperymentów i przykładowe 1000 próbek wygenerowanych za pomocą Państwa metody.\n",
    "Bardzo proszę żeby jak zwykle zwracali mi Państwo archiwum zip, jak zwykle proszę też o zastosowanie się do instrukcji:\n",
    "- Archiwum powinno być nazwane jak ostatnio poniedzialek/piatek_nazwisko1_nazwisko2.zip (lub nazwa drużyny)\n",
    "- W archiwum proszę bez zbędnych podfolderów umieścić pliki ze swoim kodem i wygenerowane obrazki nazwane odpowiednio poniedzialek_nazwisko1_nazwisko2.pt (lub nazwa drużyny)\n",
    "- Wygenerowane obrazki, proszę zapisywać po prostu w formie torchowego tensora (na cpu, po detach, czyli np. wykonując komendę torch.save(generated_imgs.cpu().detach(),\"poniedzialek_nazwisko1_nazwisko2.pt\") ). Tensor zgodnie z konwencją powinien mieć wymiary [1000, 3, 32, 32]\n",
    "\n",
    "Ewaluacja:\n",
    "- Wygenerowane obrazki porównywał będę do zbioru testowego za pomocą metryki Frechet Inception Distance o której wspominałem na ćwiczeniach. Jeżeli chcieliby Państwo z niej skorzystać do ewaluacji swoich modeli, to odsyłam do repozytorium z wygodną implementacją: https://github.com/mseitzer/pytorch-fid\n",
    "- W zbiorze testowym obrazki mają ten sam rozkład klas co w treningowym\n",
    "- Proszę pamiętać o denormalizacji próbek :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Specify the path to the zip file\n",
    "zip_file_path = '/content/trafic_32.zip'\n",
    "\n",
    "# Specify the directory to extract the contents to\n",
    "extract_dir = '/content/data/'\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Check the extracted directory structure\n",
    "!ls {extract_dir}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "LATENT_DIMENSIONS = 100\n",
    "EPOCHS = 15\n",
    "\n",
    "LEARNING_RATE = 0.00015\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))\n\u001b[1;32m      4\u001b[0m ])\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39mNUM_WORKERS, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# train_set, val_set = torch.utils.data.random_split(dataset, [45000, 5000])\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:145\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m    144\u001b[0m classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m--> 145\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextensions \u001b[38;5;241m=\u001b[39m extensions\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:189\u001b[0m, in \u001b[0;36mDatasetFolder.make_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/datasets/folder.py:87\u001b[0m, in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(target_dir):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, _, fnames \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollowlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fnames):\n\u001b[1;32m     89\u001b[0m         path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, fname)\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:413\u001b[0m, in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;66;03m# Issue #23605: os.path.islink() is used instead of caching\u001b[39;00m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;66;03m# entry.is_symlink() result during the loop on os.scandir() because\u001b[39;00m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;66;03m# the caller can replace the directory entry during the \"yield\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# above.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m followlinks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m islink(new_path):\n\u001b[0;32m--> 413\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m walk(new_path, topdown, onerror, followlinks)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# Recurse into sub-directories\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m new_path \u001b[38;5;129;01min\u001b[39;00m walk_dirs:\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:352\u001b[0m, in \u001b[0;36mwalk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# We may not have read permission for top, in which case we can't\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# get a list of the files the directory contains.  os.walk\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# always suppressed the exception then, rather than blow up for a\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# minor reason when (say) a thousand readable directories are still\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# left to visit.  That logic is copied here.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# Note that scandir is global in this module due\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# to earlier import-*.\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     scandir_it \u001b[38;5;241m=\u001b[39m \u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m onerror \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=DATA_PATH, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "# train_set, val_set = torch.utils.data.random_split(dataset, [45000, 5000])\n",
    "\n",
    "print(dataset.classes)\n",
    "print(len(dataset.classes))\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dimension, hidden_dimension):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.convolution1 = nn.Conv2d(input_dimension, hidden_dimension, kernel_size=4, stride=2, padding=1)\n",
    "        self.convolution2 = nn.Conv2d(hidden_dimension, hidden_dimension*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.convolution3 = nn.Conv2d(hidden_dimension*2, hidden_dimension*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.convolution4 = nn.Conv2d(hidden_dimension*4, 1, kernel_size=4, stride=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.convolution1(x))\n",
    "        x = self.relu(self.convolution2(x))\n",
    "        x = self.relu(self.convolution3(x))\n",
    "        x = self.sigmoid(self.convolution4(x))\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dimension, hidden_dimension, output_dimension):\n",
    "        super(Generator, self).__init__()\n",
    "        self.convtrans1 = nn.ConvTranspose1d(latent_dimension, hidden_dimension*4, kernel_size=4, stride=1, padding=0)\n",
    "        self.convtrans2 = nn.ConvTranspose1d(hidden_dimension*4, hidden_dimension*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.convtrans3 = nn.ConvTranspose1d(hidden_dimension*2, hidden_dimension, kernel_size=4, stride=2, padding=1)\n",
    "        self.convtrans4 = nn.ConvTranspose1d(hidden_dimension, output_dimension, 4, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.convtrans1(x))\n",
    "        x = self.relu(self.convtrans2(x))\n",
    "        x = self.relu(self.convtrans3(x))\n",
    "        x = self.tanh(self.convtrans4(x))\n",
    "        return x\n",
    "\n",
    "# batch normalisation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(LATENT_DIMENSIONS, 64, 3).to(device)\n",
    "discriminator = Discriminator(3, 64).to(device)\n",
    "\n",
    "optimizer_generator = optim.Adam(generator.parameters(), lr=0.00015, betas=(BETA1, BETA2))\n",
    "optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.00015, betas=(BETA1, BETA2))\n",
    "\n",
    "scheduler_generator = optim.lr_scheduler.ExponentialLR(optimizer_generator, GAMMA)\n",
    "scheduler_discriminator = optim.lr_scheduler.ExponentialLR(optimizer_discriminator, GAMMA)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "noise = torch.randn(16, LATENT_DIMENSIONS, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_losses = []\n",
    "D_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # For each batch in the dataloader\n",
    "    discriminator_fake_acc = []\n",
    "    discriminator_real_acc = []\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        # Format batch\n",
    "        real_images = data[0].to(device)\n",
    "        b_size = real_images.size(0)\n",
    "        label = torch.ones((b_size,), dtype=torch.float, device=device) # Setting labels for real images\n",
    "        # Forward pass real batch through D\n",
    "        output = discriminator(real_images).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        error_discriminator_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        discriminator_real_acc.append(output.mean().item())\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, LATENT_DIMENSIONS,device=device)\n",
    "        # Generate fake image batch with Generator\n",
    "        fake_images = generator(noise)\n",
    "        label_fake = torch.zeros((b_size,), dtype=torch.float, device=device)\n",
    "        # Classify all fake batch with Discriminator\n",
    "        output = discriminator(fake_images.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        error_discriminator_fake = criterion(output, label_fake)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        discriminator_fake_acc.append(output.mean().item())\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        error_discriminator = error_discriminator_real + error_discriminator_fake\n",
    "        error_discriminator.backward()\n",
    "        # Update D\n",
    "        optimizer_discriminator.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        optimizer_generator.zero_grad()\n",
    "        label = torch.ones((b_size,), dtype=torch.float, device=device)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake_images).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        error_generator = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        error_generator.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Output training stats\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(error_generator.item())\n",
    "        D_losses.append(error_discriminator.item())\n",
    "\n",
    "    print(f\"Epoch: {epoch}, discrimiantor fake error: {np.mean(discriminator_fake_acc):.3}, discriminator real acc: {np.mean(discriminator_real_acc):.3}\")\n",
    "    scheduler_generator.step()\n",
    "    scheduler_discriminator.step()\n",
    "    if epoch % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = generator(noise).detach().cpu()\n",
    "        grid = torchvision.utils.make_grid(fake)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.title(f\"Generations\")\n",
    "        plt.imshow(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
